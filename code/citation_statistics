import os
import time
import requests
import logging
import re
import pandas as pd
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
from pdfminer.high_level import extract_text
from transformers import pipeline
from nltk.sentiment import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import CountVectorizer
from textblob import TextBlob
import networkx as nx

# 配置日志
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger()

# 配置部分
TARGET_KEYWORDS = ["Deep Residual Learning", "He, Kaiming", "ResNet"]  # 目标文章的关键词
DOWNLOAD_DIR = "./downloaded_papers"  # PDF文件下载目录
NO_ACCESS_FILE = "./no_access_papers.txt"  # 记录无法下载的文章
OUTPUT_EXCEL_FILE = "./author_info_with_context_and_NIW.xlsx"  # 输出的 Excel 文件
MAX_RETRIES = 3  # 最大重试次数
TIMEOUT = 60  # 请求超时时间
MAX_WORKERS = 4  # 下载并发线程数

# 关键词库：包括夸赞词汇、扩展词汇和方法词汇
PRAISE_KEYWORDS = ['groundbreaking', 'innovative', 'important', 'novel', 'pioneering', 'revolutionary', 'critical', 'key contribution', 'major breakthrough', 'significant impact', 'influential', 'state-of-the-art']
EXTENSION_KEYWORDS = ['extend', 'build upon', 'based on', 'on the foundation of', 'as proposed by', 'modification of', 'adapting', 'improving', 'enhancing', 'applying', 'further study', 'derived from', 'inspired by']
APPLICATION_KEYWORDS = ['applied', 'implementation', 'use of', 'practical application', 'real-world usage', 'test on', 'experiments with']

# ========== 情感分析的增强部分 ==========
def analyze_sentiment(text):
    """使用TextBlob和VADER分析情感"""
    blob = TextBlob(text)
    # 使用VADER进行情感分析，返回积极、消极、中立的情感评分
    sia = SentimentIntensityAnalyzer()
    sentiment_score = sia.polarity_scores(text)
    # 判断情感倾向
    if sentiment_score['compound'] >= 0.05:
        return "正面"
    elif sentiment_score['compound'] <= -0.05:
        return "负面"
    else:
        return "中立"

def analyze_sentiment_advanced(text):
    """使用预训练的BERT模型进行更准确的情感分析"""
    sentiment_pipeline = pipeline("sentiment-analysis")
    result = sentiment_pipeline(text)
    return result[0]['label']  # 返回模型的情感标签（POSITIVE/NEGATIVE）

# ========== 构建引用图谱 ==========
def build_citation_network(papers):
    """构建引用网络图谱"""
    G = nx.Graph()
    for paper in papers:
        title = paper.get("citingPaper", {}).get("title", "Unknown_Title")
        citing_paper_id = paper.get("citingPaper", {}).get("id", "")
        
        citing_paper_url = paper.get("citingPaper", {}).get("url", "")
        authors, institutions, email = extract_authors_and_email(citing_paper_url)
        
        # 添加文章节点
        G.add_node(title, authors=authors, institutions=institutions, email=email)
        
        # 添加与被引用论文的连接（引用关系）
        cited_paper = paper.get("citedPaper", {}).get("title", "Unknown_Cited_Paper")
        G.add_edge(citing_paper_id, cited_paper, relationship="cites")

    return G

# ========== 自动化评估标准扩展 ==========
def extract_keywords_from_context(context):
    """从引用内容中提取关键词，用于进一步分析"""
    vectorizer = CountVectorizer(stop_words='english')
    X = vectorizer.fit_transform([context])
    keywords = vectorizer.get_feature_names_out()
    return keywords

# ========== 提取作者信息函数 ==========
def extract_authors_and_email(paper_url):
    """从论文页面提取作者、单位和邮箱信息"""
    try:
        response = requests.get(paper_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        authors = []
        email = ""
        for author_tag in soup.find_all('a', class_="author"):
            author_name = author_tag.get_text(strip=True)
            authors.append(author_name)

        # 查找可能的通讯邮箱
        email_tag = soup.find('a', href=re.compile("mailto:"))
        if email_tag:
            email = email_tag.get('href').replace('mailto:', '')

        # 假设单位信息在某个固定的标签中
        institutions = []
        institution_tags = soup.find_all('span', class_='institution')
        for tag in institution_tags:
            institutions.append(tag.get_text(strip=True))

        return authors, institutions, email

    except Exception as e:
        logger.warning(f"无法从 {paper_url} 提取作者信息: {e}")
        return [], [], ""

# ========== 从PDF中提取引用上下文 ==========
def extract_references_from_pdf(file_path, target_keywords):
    """从PDF中提取引用上下文（前后句）"""
    try:
        text = extract_text(file_path)
        if not text:
            logger.warning(f"未能提取文本: {file_path}")
            return []

        sentences = re.split(r'(?<=[.!?])\s+', text)
        references = []
        for i, sentence in enumerate(sentences):
            if any(keyword.lower() in sentence.lower() for keyword in target_keywords):
                prev_sentence = sentences[i - 1] if i > 0 else ""
                next_sentence = sentences[i + 1] if i < len(sentences) - 1 else ""
                references.append((prev_sentence.strip(), sentence.strip(), next_sentence.strip()))
        
        return references

    except Exception as e:
        logger.warning(f"无法提取引用上下文: {file_path}, 错误: {e}")
        return []

# ========== 判断引用是否有利于NIW ==========
def assess_NIW_relevance(reference):
    """评估引用内容是否有利于NIW申请"""
    prev_sentence, curr_sentence, next_sentence = reference

    # 检查是否夸赞论文
    for keyword in PRAISE_KEYWORDS:
        if keyword in curr_sentence.lower():
            return "有利", f"引用内容包含正面评价或夸赞被引用论文的创新性：{curr_sentence}"

    # 检查是否基于被引用论文的研究
    for keyword in EXTENSION_KEYWORDS:
        if keyword in curr_sentence.lower():
            return "有利", f"引用内容表明基于被引用论文的基础展开研究：{curr_sentence}"

    # 检查是否是应用相关的引用
    for keyword in APPLICATION_KEYWORDS:
        if keyword in curr_sentence.lower():
            return "有利", f"引用内容表明该研究在实际应用中有所拓展或使用：{curr_sentence}"

    # 使用情感分析判断是否夸赞被引用论文
    sentiment = analyze_sentiment_advanced(curr_sentence)
    if sentiment == "POSITIVE":
        return "有利", f"引用内容表现为积极的情感评价：{curr_sentence}"

    # 如果没有明显的夸赞、扩展或应用，判断为不利或不明确
    return "不明确", f"引用内容未明显表明对被引用论文的扩展或正面评价：{curr_sentence}"

# ========== 下载PDF并处理 ==========
def download_papers_and_extract_info(papers, download_dir):
    """下载论文并提取作者信息以及引用内容"""
    os.makedirs(download_dir, exist_ok=True)
    no_access = []
    data = []

    def process_paper(paper):
        title = paper.get("citingPaper", {}).get("title", "Unknown_Title")
        pdf_url = paper.get("citingPaper", {}).get("openAccessPdf", {}).get("url", "")
        
        if not pdf_url:
            logger.info(f"尝试从多个来源获取 PDF: {title}")
            pdf_url = find_pdf_url_from_alternatives(paper)  # 其他来源的方法
        
        if not pdf_url:
            logger.warning(f"没有找到可用的PDF链接: {title}")
            no_access.append(title)
            return

        file_name = f"{title.replace(' ', '_')[:50]}.pdf"
        file_path = os.path.join(download_dir, file_name)

        download_pdf(pdf_url, file_path)  # 下载PDF

        # 提取引用文献中的作者信息
        citing_paper_url = paper.get("citingPaper", {}).get("url", "")
        authors, institutions, email = extract_authors_and_email(citing_paper_url)

        # 提取引用上下文
        references = extract_references_from_pdf(file_path, TARGET_KEYWORDS)

        for reference in references:
            prev_sentence, curr_sentence, next_sentence = reference
            # 评估该引用是否有利于NIW
            niw_relevance, reason = assess_NIW_relevance(reference)

            for author, institution in zip(authors, institutions):
                data.append({
                    '引用文章': title,
                    '作者': author,
                    '单位': institution,
                    '联系邮箱': email,
                    '引用上下文': f"前句: {prev_sentence}, 当前句: {curr_sentence}, 后句: {next_sentence}",
                    '是否有利于NIW': niw_relevance,
                    '判断依据': reason
                })

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        executor.map(process_paper, papers)

    # 输出到Excel文件
    df = pd.DataFrame(data)
    df.to_excel(OUTPUT_EXCEL_FILE, index=False)
    logger.info(f"处理完成，结果保存到 {OUTPUT_EXCEL_FILE}")

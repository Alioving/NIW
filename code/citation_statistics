import os
import time
import requests
import logging
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from concurrent.futures import ThreadPoolExecutor
from webdriver_manager.chrome import ChromeDriverManager
from PyPDF2 import PdfReader
from pdfminer.high_level import extract_text
from pymupdf import fitz  # PyMuPDF
import pytesseract
from PIL import Image
import random
import argparse
import re

# 配置日志
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger()

# ========== 配置部分 ==========
TARGET_KEYWORDS = ["Deep Residual Learning", "He, Kaiming", "ResNet"]  # 目标文章的关键词
DOWNLOAD_DIR = "./downloaded_papers"  # PDF文件下载目录
NO_ACCESS_FILE = "./no_access_papers.txt"  # 记录无法下载的文章
MAX_RETRIES = 3  # 最大重试次数
TIMEOUT = 60  # 请求超时时间
MAX_WORKERS = 4  # 下载并发线程数

# ========== Selenium WebDriver 配置 ==========
def get_driver():
    """获取 Selenium WebDriver，自动管理驱动"""
    options = Options()
    options.headless = True  # 启用无头模式
    driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)
    return driver

# ========== 下载PDF相关函数 ==========
def retry_request(url, retries=MAX_RETRIES):
    """带重试机制的请求"""
    for _ in range(retries):
        try:
            response = requests.get(url, timeout=TIMEOUT)
            response.raise_for_status()
            return response
        except Exception as e:
            logger.warning(f"请求失败: {e}. 重试中...")
            time.sleep(random.uniform(1, 3))  # 随机延迟避免被封禁
    logger.error(f"请求失败: {url} 超过最大重试次数")
    return None

def download_pdf(pdf_url, file_path):
    """下载 PDF 文件"""
    try:
        response = retry_request(pdf_url)
        if response:
            with open(file_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            logger.info(f"[Success] 下载完成: {file_path}")
        else:
            logger.error(f"[Error] 无法下载 PDF: {file_path}")
    except Exception as e:
        logger.error(f"[Error] 下载失败: {file_path}, 错误: {e}")

# ========== 从不同网站获取PDF链接 ==========
def get_pdf_from_researchgate(title):
    """从 ResearchGate 获取 PDF 链接"""
    search_url = f"https://www.researchgate.net/search?q={title}"
    response = retry_request(search_url)
    if response:
        soup = BeautifulSoup(response.text, 'html.parser')
        pdf_link = None
        for link in soup.find_all('a', href=True):
            if 'pdf' in link['href'].lower():
                pdf_link = link['href']
                break
        return pdf_link
    return None

def get_pdf_from_core(title):
    """从 CORE 获取 PDF 链接"""
    search_url = f"https://core.ac.uk/search?q={title}"
    response = retry_request(search_url)
    if response:
        soup = BeautifulSoup(response.text, 'html.parser')
        pdf_link = None
        for link in soup.find_all('a', href=True):
            if 'pdf' in link['href'].lower():
                pdf_link = link['href']
                break
        return pdf_link
    return None

def get_pdf_from_ssrn(title):
    """从 SSRN 获取 PDF 链接"""
    search_url = f"https://www.ssrn.com/en/search/{title}"
    driver = get_driver()
    driver.get(search_url)
    time.sleep(3)  # 等待网页加载
    pdf_link = None
    pdf_elements = driver.find_elements(By.TAG_NAME, 'a')
    for element in pdf_elements:
        if 'pdf' in element.get_attribute('href').lower():
            pdf_link = element.get_attribute('href')
            break
    driver.quit()
    return pdf_link

# ========== 提取PDF文本 ==========
def extract_text_from_pdf(file_path):
    """从 PDF 文件中提取文本（使用 PyMuPDF）"""
    try:
        doc = fitz.open(file_path)
        text = ""
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            text += page.get_text("text")
        return text
    except Exception as e:
        logger.error(f"无法提取文本: {file_path}, 错误: {e}")
        return ""

def extract_text_from_image_pdf(file_path):
    """从图片PDF中提取文本（使用 OCR）"""
    try:
        images = convert_from_path(file_path)  # 需要安装 pdf2image 库
        text = ""
        for image in images:
            text += pytesseract.image_to_string(image)
        return text
    except Exception as e:
        logger.error(f"OCR 提取失败: {file_path}, 错误: {e}")
        return ""

def extract_references_from_pdf(pdf_dir, target_keywords):
    """从 PDF 中提取引用目标文章的上下文"""
    logger.info("[Info] 开始解析 PDF 文件...")
    for file in os.listdir(pdf_dir):
        if not file.endswith(".pdf"):
            continue

        file_path = os.path.join(pdf_dir, file)
        text = extract_text_from_pdf(file_path)
        if not text:
            text = extract_text_from_image_pdf(file_path)

        if text:
            sentences = re.split(r'(?<=[.!?])\s+', text)
            for i, sentence in enumerate(sentences):
                if any(keyword.lower() in sentence.lower() for keyword in target_keywords):
                    prev_sentence = sentences[i - 1] if i > 0 else ""
                    next_sentence = sentences[i + 1] if i < len(sentences) - 1 else ""
                    logger.info(f"\n[Found Reference] 文件: {file}")
                    logger.info(f"  上一句: {prev_sentence.strip()}")
                    logger.info(f"  当前句: {sentence.strip()}")
                    logger.info(f"  下一句: {next_sentence.strip()}\n")

# ========== 下载PDF并处理 ==========
def download_papers_and_extract_references(papers, download_dir):
    """下载论文并提取引用内容"""
    os.makedirs(download_dir, exist_ok=True)
    no_access = []
    
    def process_paper(paper):
        title = paper.get("citingPaper", {}).get("title", "Unknown_Title")
        pdf_url = paper.get("citingPaper", {}).get("openAccessPdf", {}).get("url", "")
        
        if not pdf_url:
            logger.info(f"尝试从多个来源获取 PDF: {title}")
            pdf_url = get_pdf_from_researchgate(title) or get_pdf_from_core(title) or get_pdf_from_ssrn(title)

        if not pdf_url:
            logger.warning(f"没有找到 PDF 链接: {title}")
            no_access.append(title)
            return
        
        file_name = f"{title.replace(' ', '_')[:50]}.pdf"
        file_path = os.path.join(download_dir, file_name)
        download_pdf(pdf_url, file_path)
        extract_references_from_pdf(download_dir, TARGET_KEYWORDS)

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        executor.map(process_paper, papers)

    with open(NO_ACCESS_FILE, "w") as f:
        for title in no_access:
            f.write(title + "\n")
    logger.info(f"[Info] 无法下载的文章已记录在 {NO_ACCESS_FILE}")

# ========== 主程序入口 ==========
def main():
    parser = argparse.ArgumentParser(description="论文下载和引用提取")
    parser.add_argument("papers", help="论文列表文件，包含引用目标论文的所有论文")
    args = parser.parse_args()

    # 假设 papers 是一个包含引用文献的列表（
